_target_: src.datamodules.language_modeling_hf.LMDataModule
dataset_name: wikitext
dataset_config_name: wikitext-103-raw-v1
tokenizer_name: gpt2
cache_dir: ${oc.env:DATA_DIR,${data_dir}}/wikitext-103/cache
block_size: 512
batch_size: 16  # per GPU
num_workers: 4
shuffle: True
__train_len: ${div_up:117920140, ${.block_size}}
